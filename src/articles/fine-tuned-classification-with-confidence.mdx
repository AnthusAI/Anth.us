---
title: "Fine-Tuned Classification With Confidence"
slug: "fine-tuned-classification-with-confidence"
date: "2024-09-01"
authors:
  - author: <a href="/ryan">Ryan Porter</a>
assistants:
  - assistant: ChatGPT
  - assistant: Claude
excerpt: "Getting the best of both worlds from LLM-based text classifiers: Easy setup, and also a confidence value."
preview_image: "./images/draft-2.png"
images:
  - ./images/draft-2.png
state: draft
---

import ArticleImage from '../components/article-image';
import { Citation, CitationsList } from 'gatsby-citation-manager';

Automated decision-making shapes our world and powers business. Should this person get a loan? Is this transaction fraudulent? Is this email spam?

These yes-or-no questions are the simplest form of what we call "classification" in machine learning.  A "classifier" makes a "prediction" about what "class" you would want for a given input: 'Would the current email user consider this email to be spam?'  It's predicting what you would think if you were to read the email yourself.  You guide it and align it with your idea of "spam" when you use the junk button.

Classification tasks go beyond simple yes-or-no decisions. For instance, a financial advisor might classify stock recommendations as "buy", "sell", or "hold". These multi-class decisions are everywhere in business: categorizing customer feedback, prioritizing leads, or diagnosing equipment faults.

Traditionally, building effective classifiers required substantial resources: powerful hardware, extensive research, and large datasets. This high barrier to entry often limited custom machine learning classifiers to well-financed organizations, like large banks assessing credit risk, tech giants filtering content, or healthcare conglomerates diagnosing diseases from medical images.

## LLMs predict the next word

The large language models (LLMs) behind generative AI are also predictors: They work by predicting the 'next' word in a sequence.  The sequence start with your prompt: "Knock, knock..."

In this case it's easy to figure out what word is likely to come next.  If someone says, "Knock, knock", the what's the next thing you're supposed to say?  "Who's there?"

We're used to asking questions and getting answers when we use LLMs through apps like ChatGPT, and we don't think about how it's predicting each word.  But look at what happens when we focus on that next-word prediction:

```
llm = ChatOpenAI(model="gpt-4o-mini").bind(logprobs=True, top_logprobs=10)
response = llm.invoke([HumanMessage(content="Knock knock.")])
logprobs = response.response_metadata['logprobs']['content'][0]['top_logprobs']
```

We send the prompt, "Knock, knock..." to GPT 4o mini, and its job is to predict the next sub-word token.  Here's how it responds:

```
Who's      0.9468
Who        0.1674
who        0.0004
 who's     0.0003
Kn         0.0001
 Who       0.0001
 who       0.0000
What's     0.0000
 quién     0.0000
谁          0.0000
```

The model gave us a list of predictions for the next token, each with the probability that it's the next token.  (The model returns logarithmic probabilities and we have to convert them to linear probabilities, which you can see in the accompanying notebook.)

We can see that the model seems to understand that it's dealing with a knock-knock joke, since it puts a 95% probability on the next word being "Who's".  Or maybe it might say, "Who is", so "Who" shows up as about a 2% probability.  Other possibilities, like responding in Spanish or Chinese, are a lot less probable.

## Predicting a word is classification

In the knock-knock example, the next word could have been anything.  What if we limit that list to just two words?  Then we can use this process to choose between those two words:

```
llm = ChatOpenAI(model="gpt-4o-mini").bind(logprobs=True, top_logprobs=8)
response = llm.invoke([
    HumanMessage(content="""
You are a text classifier for sentiment analysis.
Valid answers: "positive", "negative", lowercase, no whitespace or punctuation.
Classify the following statement as either 'positive' or 'negative':
"This article about LLM classifiers helped me to understand."
""")])
logprobs = response.response_metadata['logprobs']['content'][0]['top_logprobs']
```

The model returns this ranked list of probabilities:

```
positive   1.0000000000
 positive  0.0000000000
-positive  0.0000000000
Positive   0.0000000000
posit      0.0000000000
_positive  0.0000000000
negative   0.0000000000
positivo   0.0000000000
```

There's some noise in that list, but it effectively chooses between the classes "positive" and "negative".  You can use this as a binary classifier for sentiment analysis.  You can start using it right away, even without any training data or anything.  If you have examples of what you want then you can make it better.  You can align it more closely to the decisions you would make.

## Changing the game

LLMs are changing the landscape for business by lowering the barrier to entry for automating decisions. By leveraging cloud-based APIs, organizations can now tap into powerful, pre-trained models without managing complex infrastructure. These models come with broad knowledge that can be applied to specific tasks, even when domain-specific data is scarce. Moreover, you can rapidly improve LLM-based classifiers through prompt engineering and fine-tuning, often with far fewer examples than traditional ML models require.

These advantages are democratizing the power of custom classification, making it accessible to a broader range of organizations and use cases. Let's explore how LLMs are reshaping this fundamental tool of machine learning.

An AI API can serve as a flexible platform for easily and cheaply creating custom classifiers, whatever your situation:

#### No training data
You can get started even if you don't have any training data at all.  You can leverage the model's generalized pre-training to make decisions.  You can get up and running quickly with basic prompt engineering and then you can align your solution over time by looking at how well it performs.

#### Some examples of what you want
Fine-tuning an LLM can significantly improve the performance of an LLM on well-defined tasks.  Even just a few dozen examples can generally align a model's decisions with what you want to see.

#### Lots of examples
If you can create thousands of examples of 'correct' decisions then you can leverage machine-learning techniques for evaluating the performance of your solutions.  You can gain the additional benefit of accurate confidence values to tell you how much you can trust each individual decision.

## Aligning the classifier to your needs

### Prompt engineering

Your first step should always be the prompt itself.  Does it clearly identify the valid classification classes?  Does the task in the prompt express the elements of the decision for your classifier?

### Completion engineering

Identifying the list of classification classes is part of your prompt, but it's the first step toward engineering the answers from the model.  The "completions", in LLM terminology.

Engineering the completions is critical for LLM-based classifiers.  A lot of popular prompt-engineering techniques, like Chain-of-Thought or Thread-of-Thought prompting, won't work when you're creating text classifiers.  That's because with LLM classifiers you're leveraging the model's ability to predict the next token, and you're mapping that prediction to your classification prediction.  If you want one step of the LLM's prediction process to map to your classification, then you need to engineer your prompts so that one token can represent the classification.  "Yes" is just one token, and so is "No".  "Positive" and "Negative" are common, and so the GPT 4o mini tokenization process represents each of those as one token.  Same for "true" and "false".

You can use longer phrases for multi-class classification, but remember that LLM costs are measured in token, and the output tokens are the expensive ones.  If you can express your classes to the model as "1", "2", "3", "4", etc, then you can cut your costs.  And there are other benefits to representing the answer with the fewest-possible tokens that we'll discuss in more detail when we talk about fine tuning.

When we start fine-tuning models to improve our classifiers, the fine-tuning process needs a way to measure how well it's doing. This measurement becomes simpler and more precise when it's only looking at one or a few tokens instead of longer phrases. It's like grading a single-word answer versus a whole essay - it's quicker and clearer to see if it's right or wrong. This simplicity makes the training process faster and uses less computer power. So, using short, single-token outputs for classification not only saves on regular usage costs but can also make the whole training process quicker and cheaper.

### Completion formatting

Did you notice above that the model had a lot of ideas for responses other than the two options that I gave it for "positive" and "negative"?  It was more likely to respond with "-positive" than with "negative".  If we're tying to make a binary classifier then we want it to always answer either "positive" or "negative".  Nothing else.

That can be a problem if you're making software that's looking for specific answers so that it can record the decision the classifier made.  A good LLM-based classifier needs to use fuzzy parsing and a retry mechanism for improving reliability.  But you don't want to depend on that stuff since it can affect accuracy and costs.

Fine-tuning is a great way to control the formatting of the answers you get from the model.  It does incur additional costs, for both training and also fine-tuned models often cost more to operate.  But it can be worth the costs to get the answer formatting exactly like you want.

Let's give it just ten manually-created examples:

```
{"messages": [{"role": "user", "content": "You are a text classifier for sentiment analysis.\nValid answers: \"positive\", \"negative\", only, lowercase, no punctuation or whitespace.\nClassify the following statement as either 'positive' or 'negative':\n\"The weather today is absolutely perfect for a picnic.\""}, {"role": "assistant", "content": "positive"}]}
{"messages": [{"role": "user", "content": "You are a text classifier for sentiment analysis.\nValid answers: \"positive\", \"negative\", only, lowercase, no punctuation or whitespace.\nClassify the following statement as either 'positive' or 'negative':\n\"I'm disappointed with the poor customer service I received.\""}, {"role": "assistant", "content": "negative"}]}
...
```

After fine-tuning GPT 4o mini with only those ten examples for 20 epochs, we re-run the example above.  Now we can see that "positive" and "negative" are now the most-likely outputs, which is what we wanted:

```
positive   1.0000000000
negative   0.0002000000
 positive  0.0001000000
-positive  0.0000000000
active     0.0000000000
absolute   0.0000000000
relative   0.0000000000
post       0.0000000000
```

Now, we'll avoid extra costs and reliability problems from unpredictable outputs.  But is it giving us the right answers?

## Accuracy

How do we know if our classifier is making the right classification decisions?  This is the part where we need some data, since the only way to measure the accuracy is if we have some examples that we know are positive and others that we know are negative.  We'll take a collection of texts that have been labeled positive or negative and we'll run our classifier on them.  For each one, we'll compare our prediction with the ground-truth labels in the data.



<div className="center-small-image">
<ArticleImage images={props.pageContext.frontmatter.images} name="llm-classifiers/freud-detector-base.png" className="full" alt="LLM-based Freud detector" />
</div>

<div className="center-small-image">
<ArticleImage images={props.pageContext.frontmatter.images} name="llm-classifiers/freud-detector-fine-tuned.png" className="full" alt="LLM-based Freud detector" />
</div>

## Drawbacks

* Higher inference costs
* Potential privacy concerns
* Explainability: The "black box" nature of large models

## References

<CitationsList citationFormat="apa" />
