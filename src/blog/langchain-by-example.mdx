---
title: "LangChain by Example"
slug: "langchain-by-example"
date: "2024-08-20"
authors:
  - author: Osledy Bazó, Derek Norrbom, and <a href="/ryan">Ryan Porter</a>
excerpt: How to orchestrate LLMs to do real work.
preview_image: "./images/draft-1.png"
images:
  - ./images/draft-1.png
state: draft
---

import BlogImage from '../components/blog-image';
import { Citation, CitationsList } from 'gatsby-citation-manager';

Don't let the articles about AI being the end of software development fool you: It's the *beginning* of a whlle new kind of software development.  But building applications with language models—from traditional ML to modern LLMs—is hard.  Building software is hard enough in the first place before the machines start talking back to you and having opinions.  One tool that's helped us a lot is LangChain, and we want to share some of what we've learned about how easy it is to use.

## Why LangChain?

We didn't start with LangChain. Our initial approach to creating "agentic" applications (a term with as many definitions as developers) was bare-bones: direct Python code for LLM API requests, no frameworks. This gave us control and deep understanding, but it came with challenges.

As we built more sophisticated applications, the complexity grew. We created systems to manage LLM chat histories, switched to LiteLLM to support various AI APIs, and wrote increasingly complex code to parse LLM responses. Soon, we were spending as much time on infrastructure as on solving our core problems.

We knew we needed a better solution, so we conducted extensive research into various frameworks. We evaluated options like CrewAI, AutoGen, and others. After careful consideration, we chose LangChain. It stood out for its ability to provide power through flexibility and control, while abstracting away many details we didn't want to handle directly. Crucially, LangChain doesn't impose rigid opinions about application architecture, unlike some alternatives that prescribe specific paradigms (such as CrewAI's concept of building applications with "crews" of "agents" with personas).

## What is LangChain?

LangChain is a framework designed to facilitate the development of applications powered by large language models (LLMs). It offers tools and abstractions to handle prompts, manage chains of logic, and parse outputs. With LangChain, you can easily create robust and flexible AI-driven workflows without dealing with low-level API calls and prompt engineering complexities.

## Getting Started: Setting Up LangChain

Before diving into examples, let's start by setting up LangChain in a Python environment. The installation process is straightforward, and you can quickly get up and running with a few simple commands.

```python
# Install necessary libraries
!pip install langchain langchain-community langchain-core langchain-openai

# Set up the OpenAI API key
from google.colab import userdata
import os
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# Show more about what's happening as it happens.
from langchain.globals import set_debug
set_debug(True)
```

Once installed, you're ready to start building with LangChain.

## Example 1: Basic Chain Invocation

Let's begin with a basic example where we create a simple chain to answer questions. This chain uses a template-based approach to generate prompts for a language model.

```python
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# Define the prompt template
template = "Answer the question: {question}"
prompt = PromptTemplate(
    template=template,
    input_variables=["question"])

# Create a simple chain using OpenAI's chat model
chain = ChatOpenAI(temperature=0.7)

# Example question
question = "What is the capital of France?"

# Invoke the chain
response = chain.invoke(question)
print(response.content)
```

The first thing that you'll see when you run this cell is the output from LangChain that we got from using `set_debug(True)`:

```
[llm/start] [llm:ChatOpenAI] Entering LLM run with input:
{
  "prompts": [
    "Human: What is the capital of France?"
  ]
}
[llm/end] [llm:ChatOpenAI] [732ms] Exiting LLM run with output:
{
  "generations": [
    [
      {
        "text": "The capital of France is Paris.",
        "generation_info": {
          "finish_reason": "stop",
          "logprobs": null
        },
        "type": "ChatGeneration",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "schema",
            "messages",
            "AIMessage"
          ],
          "kwargs": {
            "content": "The capital of France is Paris.",
            "additional_kwargs": {
              "refusal": null
            },
            "response_metadata": {
              "token_usage": {
                "completion_tokens": 7,
                "prompt_tokens": 14,
                "total_tokens": 21
              },
              "model_name": "gpt-3.5-turbo-0125",
              "system_fingerprint": null,
              "finish_reason": "stop",
              "logprobs": null
            },
            "type": "ai",
            "id": "run-710678c5-1a80-4a8c-8fa3-14a9d1e7e0eb-0",
            "usage_metadata": {
              "input_tokens": 14,
              "output_tokens": 7,
              "total_tokens": 21
            },
            "tool_calls": [],
            "invalid_tool_calls": []
          }
        }
      }
    ]
  ],
  "llm_output": {
    "token_usage": {
      "completion_tokens": 7,
      "prompt_tokens": 14,
      "total_tokens": 21
    },
    "model_name": "gpt-3.5-turbo-0125",
    "system_fingerprint": null
  },
  "run": null
}
```

And then you'll see the answer, extracted from the `content` of the first "generation" from the LLM:

```
The capital of France is Paris.
```

## Example 2: Using an Output Parser

That's a great response!  But it's not the actual answer to the question, it is?  It's a sentence, but what we really wanted was a city name.

Welcome to output parsers!  An application built around LLMs has to be able to get structured data back from them.  There are various ways to do that, and the landscape changes constantly, but in general you need code for interpreting what the language model says.  Often, you need to structure it by extracting details from it.

Here's a simple output parser that plucks out those two named entities.  This could work if you had a model fine-tuned to reliably produce sentences in this format:

```python
import re

# Define a simple output parser
def extract_capital(output):
    match = re.search(r"The capital of [a-zA-Z\s]+ is ([a-zA-Z\s]+)\.", output)
    return match.group(1) if match else None

# Example response from the model
output = "The capital of France is Paris."

# Parse the output
capital = extract_capital(output)
print(f"Extracted Capital: {capital}")
```

The output of this cell:

```
Extracted Capital: Paris
```

## Example 3: Applying Chains to DataFrame Rows

Now, let's move on to a more complex and practical example. Suppose you have a DataFrame containing questions, and you want to process each row.  This is a common task in data processing workflows where you need to generate or enrich data using language models.  With LangChain, you can use the `chain.invoke()` method.

```python
import pandas as pd

# Example DataFrame
data = {
    'id': [1, 2, 3],
    'text': [
        "What is the capital of France?",
        "Who is the president of the United States?",
        "What is the population of Japan?"
    ]
}
df = pd.DataFrame(data)

# Define a function to process each row
def process_row(row):
    question = row['text']
    result = chain.invoke(question)
    return result.content  # Adjust according to your chain's output format

# Apply the function to each row in the DataFrame
df['response'] = df.apply(process_row, axis=1)

# Display the DataFrame with the responses
print(df)
```

The output of this cell:

```
   id                                        text  \
0   1              What is the capital of France?   
1   2  Who is the president of the United States?   
2   3            What is the population of Japan?   

                                            response  
0                    The capital of France is Paris.  
1  As of October 2021, the President of the Unite...  
2  As of 2021, the population of Japan is approxi...  
```

## Example 4: Combining Output Parsers with DataFrames

In some cases, you might want to combine the use of output parsers with DataFrame processing. This allows you to extract and structure data from the model's output directly within your DataFrame.

```python
# Extend the process_row function to include parsing
def process_and_parse_row(row):
    question = row['text']
    result = chain.invoke(question)
    output = result.content
    parsed_output = extract_capital(output)  # Use the previously defined parser
    return parsed_output

# Apply the enhanced function to each row
df['capital'] = df.apply(process_and_parse_row, axis=1)

# Display the updated DataFrame
print(df)
```

The output of this cell:

```
   id                                        text  \
0   1              What is the capital of France?   
1   2  Who is the president of the United States?   
2   3            What is the population of Japan?   

                                            response capital  
0                    The capital of France is Paris.   Paris  
1  As of October 2021, the President of the Unite...    None  
2  As of 2021, the population of Japan is approxi...    None  
```

## Generalized parser

Using a generalized parser to take into account different outputs

```python
import re

def generalized_output_parser(output):
    # Try to extract a capital city
    capital_match = re.search(r"The capital of [a-zA-Z\s]+ is ([a-zA-Z\s]+)\.", output)
    if capital_match:
        return capital_match.group(1)

    # Try to extract the president's name
    president_match = re.search(r"President of the United States is ([a-zA-Z\s]+)\.", output)
    if president_match:
        return president_match.group(1)

    population_match = re.search(r"population of [a-zA-Z\s]+ is (?:estimated to be|approximately|around|about)?\s*([\d,\.]+)\s*million", output)

    if population_match:
        return population_match.group(1) + " million"

    # If nothing matches, return None
    return None

# Applying the parser to the DataFrame
def process_and_parse_row(row):
    question = row['text']
    result = chain.invoke(question)  # Assuming chain.invoke returns a text output
    # Access the content of the AIMessage object using .content
    output = result.content
    parsed_output = generalized_output_parser(output)
    return parsed_output

# Apply the generalized parser to each row
df['parsed_info'] = df.apply(process_and_parse_row, axis=1)

# Display the DataFrame
print(df[['response', 'parsed_info']])
```

The output of this cell:

```
                                            response    parsed_info
0                    The capital of France is Paris.          Paris
1  As of October 2021, the President of the Unite...      Joe Biden
2  As of 2021, the population of Japan is approxi...  126.3 million
```

## Conclusion

LangChain provides a powerful and flexible way to integrate language models into your data workflows. Whether you're working with unstructured text, automating responses, or extracting specific information, LangChain's tools and abstractions simplify the process.

In this article, we've explored practical examples of how to use LangChain, from basic chain invocations to more complex tasks like applying chains to DataFrame rows and parsing outputs. With these examples, you're well-equipped to start building your own intelligent applications using LangChain.

## References

<CitationsList citationFormat="apa" />
