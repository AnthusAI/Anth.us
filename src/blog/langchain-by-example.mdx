---
title: "LangChain by Example"
slug: "langchain-by-example"
date: "2024-08-20"
authors:
  - author: Osledy Bazó
  - author: Derek Norrbom
  - author: <a href="/ryan">Ryan Porter</a>
assistants:
  - assistant: ChatGPT
  - assistant: Claude
excerpt: How to orchestrate LLMs to do real work.
preview_image: "./images/draft-1.png"
images:
  - ./images/draft-1.png
  - ./images/LangChain/agentic-framework-logos.png
  - ./diagrams/LangChain/API-multiplexer.png
state: published
---

import BlogImage from '../components/blog-image';
import { Citation, CitationsList } from 'gatsby-citation-manager';

You have problems, and you know that AI can help solve them. You also know that you need software to get you there. But automating AI can be a challenge. Software development was hard enough before the machines started talking back and having opinions! Now, it's even more difficult because the components you're building on can be unpredictable, producing different results at different times.

That's where we come in. Our team specializes in making these advanced tools accessible. One powerful tool we're excited to share with you is LangChain. LangChain helps you stay focused on solving your problems and creating effective solutions, rather than getting bogged down in the complexities of managing the underlying AI technology.

We'd like to share some examples of practical solutions we've found using LangChain:

* [Use AI models from any company's AI API](#ai-api-multiplexer) - Using AI models on-demand as interchangeable components within larger software architectures, connecting to various AI API providers.
* Processing large volumes of data, such as analyzing tens of thousands of call center transcripts to generate examples for fine-tuning LLMs.
* Building robust AI components that can validate LLM output and retry with hints, improving reliability.
* Creating multi-step processes that break down complex tasks, allowing the use of more cost-effective AI models while achieving better results – a key principle in leveraging AI for practical solutions.

These are just a few examples of what's possible. Whether you're tackling business challenges, working on academic research, or exploring AI as a hobby, LangChain can help you harness the power of language models more effectively.

With so many approaches available for building AI applications, you might wonder: Why LangChain? Let's explore what makes this tool stand out and how it can streamline your AI development process.

## Why LangChain?

We didn't start with LangChain. Our initial approach to creating "agentic" applications (a term with as many definitions as developers) was bare-bones: Python code for LLM API requests, no frameworks. This gave us control and deep understanding, but it came with challenges.

As we built more sophisticated applications, the complexity grew. We created systems to manage LLM chat histories, switched to [LiteLLM](https://litellm.vercel.app/) to support various AI APIs, and wrote increasingly complex code to parse LLM responses. Soon, we were spending as much time on infrastructure as on solving our core problems.

We knew we needed a better solution, so we conducted extensive research into various frameworks. We evaluated options like CrewAI, AutoGen, and others.

<div className="center-full-image">
<BlogImage images={props.pageContext.frontmatter.images} name="LangChain/agentic-framework-logos.png" className="full" alt="LangChain is an AI API multiplexer." />
</div>

After careful consideration, we chose LangChain. It stood out for its ability to provide power through flexibility and control, while abstracting away many details we didn't want to handle directly. Crucially, LangChain doesn't impose rigid opinions about application architecture, unlike some alternatives that prescribe specific paradigms (such as CrewAI's concept of building applications with "crews" of "agents" with personas).<Citation
  data={{
    type: "webpage",
    "container-title": "CREW AI Documentation",
    title: "Core Concepts: Agents",
    author: "CREW AI",
    URL: "https://docs.crewai.com/core-concepts/Agents/",
    issued: { 'date-parts': [[2024, 8, 21]] },
  }}
/>

## What is LangChain?

LangChain is a framework for building applications with LLMs. It handles prompts. It manages logic chains. It parses outputs. With LangChain, you focus on AI workflows for solving your use cases.  You don't have to mess with API calls or the differences between APIs.  It's a simple abstraction that's easy to understand. It's powerful through its flexibility. It's well used and maintained by masses of other developers.  And it works.

Let's look at some examples of ways you can use it to solve problems.

## <a name="ai-api-multiplexer"></a>Example 1: Use AI models from any company's AI API

OpenAI gives you example code for calling their API.  It's easy.  Really.

It starts to get harder when you need to connect the same code to more than one AI API.  It's not just a matter of changing a URL and an API key: Different APIs have different input and output structures.  Even if you use something like AWS Bedrock that can connect to many different models, your code has to account for some models accepting a `top_p` parameter (Anthropic Claude<Citation
  data={{
    type: "webpage",
    "container-title": "Amazon Bedrock Documentation",
    title: "Model Parameters: Anthropic Claude Text Completion",
    author: "AWS",
    URL: "https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-text-completion.html",
    issued: { 'date-parts': [[2024, 8, 21]] },
  }}
/>, Mistral AI<Citation
  data={{
    type: "webpage",
    "container-title": "Amazon Bedrock Documentation",
    title: "Model Parameters: Mistral Text Completion",
    author: "AWS",
    URL: "https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral-text-completion.html",
    issued: { 'date-parts': [[2024, 8, 21]] },
  }}
/>, Meta LLama<Citation
  data={{
    type: "webpage",
    "container-title": "Amazon Bedrock Documentation",
    title: "Model Parameters: Meta",
    author: "AWS",
    URL: "https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html",
    issued: { 'date-parts': [[2024, 8, 21]] },
  }}
/>) while others use `topP` (Amazon Titan<Citation
  data={{
    type: "webpage",
    "container-title": "Amazon Bedrock Documentation",
    title: "Model Parameters: Titan Text",
    author: "AWS",
    URL: "https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html",
    issued: { 'date-parts': [[2024, 8, 21]] },
  }}
/>>, A21 Labs Jurrasic-2<Citation
  data={{
    type: "webpage",
    "container-title": "Amazon Bedrock Documentation",
    title: "Model Parameters: Jurassic-2",
    author: "AWS",
    URL: "https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-jurassic2.html",
    issued: { 'date-parts': [[2024, 8, 21]] },
  }}
/>).  Some support `top_k` and some don't.  Some support `seed` and some don't.  Some have different request formats, with different ways of defining "functions" or "tools".

Who will keep up with all of it?  You shouldn't.  That's what LangChain is for: You can connect your code through LangChain to many different AI APIs.  Your code can worry about its problems while LangChain worries about those details.

<div className="center-full-image">
<BlogImage images={props.pageContext.frontmatter.images} name="API-multiplexer.png" className="full" alt="LangChain is an AI API multiplexer." />
</div>

LangChain isn't the only option if all you want is to connect your same code to many different AI APIs.  Another great option is LiteLLM, which makes every AI API look like the OpenAI API so that you can use the same code for everything.  That strategy worked for us — to a point. When we started writing lots of code for managing conversation histories and parsing model outputs we realized that we were reinventing wheels when we should have been focused on solving problems.  LangChain seemed like the best option for abstracting those details without forcing us to conform to a framework's paradigm.

Here's a minimal LangChain AI API request:

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model_name="gpt-4o")

model.invoke("Is the Sagrada Família located in Barcelona?").content
```

Output:

```
Yes, the Sagrada Família is located in Barcelona, Spain. It is a large Roman Catholic basilica designed by the renowned architect Antoni Gaudí. Construction of the Sagrada Família began in 1882 and it remains unfinished to this day, although work continues with the aim of completing it in the coming years. The basilica is one of Barcelona's most iconic landmarks and a UNESCO World Heritage Site.
```

Simple!

If you do this same request [with the OpenAI API](https://platform.openai.com/docs/guides/chat-completions/overview) then you get the response in `message = completion.choices[0].message.content`.  LangChain encapsulates those details so that you can think about what you're doing instead of working to accommodate the API.

That's useful because LangChain requests to other AI APIs, like AWS Bedrock, Anthropic, or Google Vertex look just like LangChain requests to OpenAI.  This example of sending a request to Claude 3.5 Sonnet looks exactly like the example above that uses OpenAI.  Even though the APIs and models have different details.

```python
from langchain_aws import ChatBedrock

model = ChatBedrock(
    model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",
)

model.invoke("Is the Sagrada Família located in Barcelona?.").content
```

Sorting out the details of the differences between the different AI APIs and models could consume hours of your time up front.  And complications related to managing those details could cost you even more time and delayed progress in the future.  Shifting responsibility for those details into LangChain keeps you focused on your goals.

### Follow along and run the examples

You can experience everything in the article yourself, for free, without setting up anything on your computer.  All the [example notebooks](https://github.com/AnthusAI/LangChain-Examples) are hosted on GitHub.  The quickest way to run them is to go to this [Google Colab notebook](https://colab.research.google.com/github/AnthusAI/LangChain-Examples/blob/main/notebooks/AI_API_Multiplexer.ipynb) for the first example.

## References

<CitationsList citationFormat="apa" />
