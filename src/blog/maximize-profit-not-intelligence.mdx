---
title: "Maximize Profit, Not Intelligence"
slug: "maximize-profit-not-intelligence"
date: "2023-12-22"
authors:
  - author: <a href="/ryan">Ryan Porter</a>
  - author: and the <a href="https://chat.openai.com/g/g-MA4ZbLQBi-assistant-alpha"><i>Anthus</i></a> GPT
tags: ['explainer', 'how-to']
excerpt: 2024 won't just be about ever-smarter artificial intelligence.  It will be about intelligently using artificial intelligence.  Unless you're an AI researcher, your goal is to maximize profit.  Not intelligence.
preview_image: "./images/classifiers/profit_per_million_GPT-4_GPT-3.5_Ada_2_BERT.png"
images:
  - ./images/classifiers/accuracy.png
  - ./images/classifiers/precision-and-recall.png
  - ./images/classifiers/ROC-curve.png
  - ./images/classifiers/confusion-word2vec.png
  - ./images/classifiers/confusion-BERT.png
  - ./images/classifiers/confusion-ada-2.png
  - ./images/classifiers/cost_GPT-4_GPT-3.5.png
  - ./images/classifiers/cost_per_million_GPT-4_GPT-3.5.png
  - ./images/classifiers/accuracy_GPT-4_GPT-3.5.png
  - ./images/classifiers/precision_recall_GPT-4_GPT-3.5.png
  - ./images/classifiers/accuracy_GPT-4_GPT-3.5_Ada_2.png
  - ./images/classifiers/cost_GPT-4_GPT-3.5_Ada_2.png
state: published
---

import BlogImage from '../components/blog-image';
import { Citation, CitationsList } from 'gatsby-citation-manager';

Picture this: you're at the helm of a startup, brimming with ideas, ready to leverage the marvels of AI, particularly the genius of GPT 4.  You set out looking for opportunities in text classifiers:<Citation
  data={{
    type: "webpage",
    "container-title": "HuggingFace",
    title: "Text Classification",
    URL: "https://huggingface.co/tasks/text-classification",
    accessed: { 'date-parts': [[2023, 12, 22]] }
  }}
/>  the kind of thing you might use to moderate posts on social media, route customers in a phone maze, direct customer emails to support agents, rate transcripts of call center recordings, etc.  Sounds straightforward, but it's tricky. Language is complex.

You went out and did your business development magic and worked out a deal!  You can make a profitable venture if you can make an automation that can classify sentences as being about Spanish immigration law or not about Spanish immigration law, with at least a 95% accuracy, at a price of less than $200 per million classifications.

No problem!  GPT 4's <a href="/blog/how-ai-agents-do-things/">function-calling</a> feature can generate structured responses!  You jump into VS Code and use GPT 4 in GitHub Copilot to craft a text classifier based on GPT 4, and it's 97% accurate when you evaluate it.  <i>AWESOME</i>, it works!

But there's a problem: You check the costs on your evaluation run, and wow, <mark>GPT 4 tokens are expensive!</mark>  It costs just over $0.40 to do 240 classifications, which means that it would cost... almost $1,700 per million classifications.  The venture would not be profitable.

This is the story of 2023: We have amazing new tools, but there's a difference between a cool proof-of-concept demo and a viable business venture.  Now, as 2024 unfolds, the narrative shifts from mere intelligence to profitability.  Once you find a use case with business value and prove the concept, you can maximize profit by reducing your costs, by finding the least-sophisticated model that will suit your task.

It's kind of like how you set the optimal price: It's the maximum price that the market will bear.  In AI applications, the path to profitability is to use the <mark>dumbest model that the problem will bear.</mark>

## Make it cheaper

One easy way to make a GPT 4 solution cheaper is to use GPT 3.5 instead.  It's a really good model.  And it's a great value compared to GPT 4, which is amazing but expensive.

You make a simple tweak to your code to swap the models and you do your evaluation run again to check the cost.  It's much better!  GPT 3.5 is an order of magnitude cheaper:

<BlogImage images={props.pageContext.frontmatter.images} name="cost_GPT-4_GPT-3.5.png" className="centered" alt="Cost of GPT 4 vs GPT 3.5 classifiers." />

The best part here is that the cost per million will be below your break-even point of $200 per million.  You're profitable! Yay!

<BlogImage images={props.pageContext.frontmatter.images} name="profit_per_million_GPT-4_GPT-3.5.png" className="centered" alt="Profit per million AI text classifications with GPT 4 versus GPT 3.5." />

But is it as accurate?  Yes, it's about the same accuracy.  It's enough.  In the evaluation phase you're measuring things like precision and recall and accuracy, and you get about the same results.

<BlogImage images={props.pageContext.frontmatter.images} name="precision_recall_GPT-4_GPT-3.5.png" className="centered" alt="Precision and recall for GPT 4 versus GPT 3.5 LLM text classifiers." />

The accuracy score is actually a little higher for GPT 3.5 but that might be related to the training data that was generated with GPT 3.5.

<BlogImage images={props.pageContext.frontmatter.images} name="accuracy_GPT-4_GPT-3.5.png" className="centered" alt="Accuracy for GPT 4 versus GPT 3.5 LLM text classifiers." />

Now you have a profitable venture.  But your margins are razor thin:

<BlogImage images={props.pageContext.frontmatter.images} name="profit_per_million_GPT-4_GPT-3.5.png" className="centered" alt="An AI model learning" />

You want to optimize for profit, right?  And in software we optimize through iteration.  Let's try to cut costs.

## Machine-learning text classifiers

This kind of algorithm that you're selling at a profit is called a "classifier", and there are a lot of ways to make them,<Citation
  data={{
    type: "webpage",
    "container-title": "Levity",
    title: "Text classifiers in Machine Learning: A practical guide",
    author: 'Patricia Orza',
    URL: "https://levity.ai/blog/text-classifiers-in-machine-learning-a-practical-guide",
    accessed: { 'date-parts': [[2023, 12, 17]] },
    issued: { 'date-parts': [[2022, 11, 16]] }
  }}
/> including ways that don't use large language models.  Let's look into making a classifier with machine learning.

One way to do it is to use a different model from OpenAI that isn't a generative text model.  It's an embeddings model, for turning text into strings of numbers.<Citation
  data={{
    type: "webpage",
    "container-title": "OpenAI",
    title: "New and improved embedding model",
    author: ['Ryan Greene','Ted Sanders','Lilian Weng','Arvind Neelakantan'],
    URL: "https://openai.com/blog/new-and-improved-embedding-model",
    accessed: { 'date-parts': [[2023, 12, 12]] },
    issued: { 'date-parts': [[2022, 12, 15]] }
  }}
/>  There are similar embeddings models from other companies, like AWS<Citation
  data={{
    type: "webpage",
    "container-title": "AWS",
    title: "Amazon Titan Embeddings is now generally available",
    URL: "https://aws.amazon.com/about-aws/whats-new/2023/09/amazon-titan-embeddings-generally-available/",
    accessed: { 'date-parts': [[2023, 12, 17]] },
    issued: { 'date-parts': [[2023, 9, 18]] }
  }}
/>.

You can use those numbers to classify the text in any way that you want if you have a lot of examples that you can use for finding patterns in the data.  Here's how:

### Step One: Crafting Numerical Fingerprints

Computers don't understand words, only numbers.  To process words with computers we have to convert them to numbers.  You could just assign every word a number, "cat" is 1, "dog" is 2, etc.  But there are more useful ways.

You can take a sentence and transform it into a numerical pattern, an 'embedding'. Imagine these embeddings as secret codes that capture the essence of each sentence numerically.<Citation
  data={{
    type: "webpage",
    "container-title": "Pinecone",
    title: "What are Vector Embeddings",
    author: "Rajat Tripathi",
    URL: "https://www.pinecone.io/learn/vector-embeddings/",
    accessed: { 'date-parts': [[2023, 12, 17]] },
  }}
/>  We have tools that measure how 'similar' these embeddings are, assigning a number to represent the degree of resemblance between any two sentences.<Citation
  data={{
    type: "webpage",
    "container-title": "Built In",
    title: "Understanding Cosine Similarity and Its Applications",
    author: "Richmond Alake",
    URL: "https://builtin.com/machine-learning/cosine-similarity",
    accessed: { 'date-parts': [[2023, 1, 19]] },
  }}
/>  So, the difference between "cat" and "dog" might be less than the distance between "cat" and "suspension bridge".

While embeddings offer rich, condensed representations of text, they don't inherently categorize sentences. How can we build a classifier out of this?

### Step Two: The Learning Machine

Enter the machine-learning model. Its job is to learn from examples - to recognize which numerical patterns correspond to our topic of interest. By examining known questions about immigration law and comparing them to unrelated sentences, the model learns to identify a benchmark for similarity. It's akin to teaching someone to recognize a specific tune among various songs.

For that, we used logistic regression, a statistical method that's like a skilled goldsmith, discerning which nuggets of text match our sought-after criteria. It operates on a simple yet powerful principle: calculating the likelihood that a new sentence belongs to one of two categories – a binary decision of 'yes' or 'no'.<Citation
  data={{
    type: "webpage",
    "container-title": "Statology",
    title: "Introduction to Logistic Regression",
    author: "Zach",
    URL: "https://www.statology.org/logistic-regression/",
    issued: { 'date-parts': [[2020, 10, 27]] },
  }}
/>

Picture logistic regression as a finely-tuned scale. It weighs the unique numerical fingerprints of words – our carefully crafted vector embeddings – and decides how 'heavy' they are in terms of relevance to our categories. By adjusting this scale, logistic regression finds the precise balance point, the exact threshold of similarity, that tips the scale towards a 'yes' or a 'no'. This isn't just a guessing game; it's a calculated decision based on the distinct patterns and relationships our embeddings reveal.

It's a simple<Citation
  data={{
    type: "webpage",
    "container-title": "Spot Intelligence",
    title: "How To Implement Logistic Regression Text Classification In Python With Scikit-learn and PyTorch",
    author: "Neri Van Otten",
    URL: "https://spotintelligence.com/2023/02/22/logistic-regression-text-classification-python/",
    accessed: { 'date-parts': [[2023, 2, 22]] },
  }}
/> yet effective tool in our arsenal, designed to take an array of "features" as inputs and produce a binary yes/no classification.  That's exactly what we need: We need a model that takes a string of numbers from the vector embeddings and produces a prediction about whether a given embedding represents a sentence about Spanish immigration law.

## Method

We <mark>generated a list of about 1,200 fake sentences using GPT 3.5</mark>, where each is labeled yes or no: Does the sentence represent a question about Spanish immigration law?  We asked for sentences in English, Spanish and Spanglish.  You can see those in the [`data/labeled.csv`](https://github.com/Anth-us/semantic-text-classification/blob/main/data/labeled.csv) file in the GitHub repository for the experiment.

We used the [Python notebook](https://github.com/Anth-us/semantic-text-classification/blob/main/Custom_Classifier.ipynb) in the repository to load that data and generate an embedding for each sentence in the file using the OpenAI Ada 2 embeddings model.  How did it perform?

<BlogImage images={props.pageContext.frontmatter.images} name="accuracy_GPT-4_GPT-3.5_Ada_2.png" className="centered" alt="Accuracy across GPT 4, GPT 3.5 and Ada 2 text classifiers." />

The accuracy improved!  Amazing!

### Cost reduction

Even more amazing is the order-of-magnitude cost reduction.  The Ada 2 model only costs $0.0001 / 1K tokens for input and none for output, compared with GPT 3.5 which costs $0.0010 / 1K tokens for input and	$0.0020 / 1K tokens for output.  The cost has dropped so low that you can't even see it on the visualization next to the cost of GPT 4.

<BlogImage images={props.pageContext.frontmatter.images} name="cost_GPT-4_GPT-3.5_Ada_2.png" className="centered" alt="Cost across GPT 4, GPT 3.5 and Ada 2 text classifiers." />

Wow!  Let's scale that to a million classifications:

<BlogImage images={props.pageContext.frontmatter.images} name="cost_per_million_GPT-4_GPT-3.5_Ada_2.png" className="centered" alt="Cost per million across GPT 4, GPT 3.5 and Ada 2 text classifiers." />

And we know why that's important.  An <mark>order-of-magnitude increase in profit</mark>:

<BlogImage images={props.pageContext.frontmatter.images} name="profit_per_million_GPT-4_GPT-3.5_Ada_2.png" className="centered" alt="Profit across GPT 4, GPT 3.5 and Ada 2 text classifiers." />

### Speed improvement

Another incredible thing is how fast machine-learning classifiers are, compared with LLMs that generate responses one token at a time.  A machine-learning classifier shifts the time required to a one-time training step, and they're virtually instant at inference time.  GPT 3.5 and GPT 4 don't require any training since they're "pre-trained", but they're <mark>super slow</mark> when they generate text compared to a machine-learning classifier when the job at hand is text classification.

<BlogImage images={props.pageContext.frontmatter.images} name="time_training_GPT-4_GPT-3.5_Ada_2.png" className="centered" alt="Time breakdown for training and inference across GPT 4, GPT 3.5 and Ada 2 text classifiers." />

## More!

Can we make it even cheaper?  Can we <mark>eliminate the OpenAI bill entirely</mark>?

Yes, we can.  OpenAI doesn't have a monopoly on vector embeddings.  There are lots of ways to do it.  We could use AWS, but currently the pricing of the Titan Embeddings model is the same as the price of the OpenAI Ada 2 model, so that won't help us with costs.

But there are algorithms we can run ourselves.  One of them is BERT.

BERT stands for Bidirectional Encoder Representations from Transformers. It revolutionized the understanding of context in language models by reading the text in both directions and building a deep context understanding.<Citation
  data={{
    type: "webpage",
    "container-title": "Medium: Towards Data Science",
    title: "BERT Explained: State of the art language model for NLP",
    author: "Rani Horev",
    URL: "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270",
    issued: { 'date-parts': [[2018, 11, 10]] },
  }}
/> BERT processes words in relation to all the other words in a sentence, rather than one-by-one in order.  It's a common embeddings model, so let's try it.

### Accuracy

BERT does just as well as Ada 2 at this task, despite being free.  <mark>Good to know!</mark>

<BlogImage images={props.pageContext.frontmatter.images} name="accuracy_GPT-4_GPT-3.5_Ada_2_BERT.png" className="centered" alt="Accuracy across GPT 4, GPT 3.5, Ada 2 and BERT text classifiers." />

### Speed

Computing the embeddings using BERT took about the same amount of time as it took to outsource the job to OpenAI.  It took a little less time.

And, like the model based on OpenAI embeddings, it's virtually instant to compute classifications with it.  All the time is in a one-time training step.

<BlogImage images={props.pageContext.frontmatter.images} name="time_training_GPT-4_GPT-3.5_Ada_2_BERT.png" className="centered" alt="Time for training and inference across GPT 4, GPT 3.5, Ada 2 and BERT text classifiers." />

### Cost

This classifier completely eliminates the OpenAI bill.  Which was already so small with Ada 2 that you can't even see it on this visualization compared with the cost of GPT 4 and GPT 3.5.  But <mark>now the cost is <b>zero</b></mark>.

<BlogImage images={props.pageContext.frontmatter.images} name="cost_per_million_GPT-4_GPT-3.5_Ada_2_BERT.png" className="centered" alt="Cost per million classifications across GPT 4, GPT 3.5, Ada 2 and BERT text classifiers." />

### Profit

And that increases our profit!  Our <mark>key performance indicator</mark>!

<BlogImage images={props.pageContext.frontmatter.images} name="profit_per_million_GPT-4_GPT-3.5_Ada_2_BERT.png" className="centered" alt="Profit across GPT 4, GPT 3.5, Ada 2 and BERT text classifiers." />

## More!

We can't really improve the price of "free".

And we can't improve on a perfect accuracy score.

What can we improve, training time?  Sure, okay, let's try Word2Vec since it's simpler than BERT.  Maybe it will be faster.

Word2Vec is a group of models that produce word embeddings by capturing the context of a word in a document.<Citation
  data={{
    type: "webpage",
    "container-title": "Medium: Towards Data Science",
    title: "Word2Vec Explained",
    author: "Vatsal",
    URL: "https://towardsdatascience.com/word2vec-explained-49c52b4ccb71",
    issued: { 'date-parts': [[2021, 7, 29]] },
  }}
/> It leverages neural networks and operates on the principle that words appearing in similar contexts have similar meanings.

Is it faster?  Yes!  It's so much faster at computing embeddings that you <mark>can't even see the blip</mark> on this visualization, next to the times that the other models require.  There's a tiny bar there on the bottom right if you squint.

<BlogImage images={props.pageContext.frontmatter.images} name="time_training_GPT-4_GPT-3.5_Ada_2_BERT_Word2Vec.png" className="centered" alt="Time for training and inference across GPT 4, GPT 3.5, Ada 2, BERT and Word2Vec text classifiers." />

Does it work, though?

<BlogImage images={props.pageContext.frontmatter.images} name="accuracy_GPT-4_GPT-3.5_Ada_2_BERT_Word2Vec.png" className="centered" alt="Accuracy across GPT 4, GPT 3.5, Ada 2, BERT and Word2Vec text classifiers." />

Oops.  No, it's significantly less accurate, at 97%.  That's accurate enough for our requirements, but it doesn't seem worth the drop in accuracy to save a few seconds in a one-time pre-training step.  It won't significantly boost our profit, and <mark>that's what matters</mark>.

Okay, let's stop while we're ahead and go with BERT.  You can't know until you try it.  Now we know.

Our goal is to use the <mark>dumbest model that the problem will bear.</mark>  To find that point, we have to go all the way to the point where we find a model that's just not good enough.  We have found that point.  It's just like finding the market price for something: You increase the price until the market shows you that the price is too high, and then you lower it to the point where you get sales.  We can do the same thing with AI solutions to find the best value.

## Try it yourself

Don't take our word for it: you can try it for yourself for free.  It's amazing that you can train and run the classifiers from a [free Google Colab notebook](https://colab.research.google.com/drive/1THEqXHIvy1IEAzVLvB0rVwwSW30jZ5Ku?usp=sharing).  To use it, just go there and set up two things:

1. Use the "Download raw file" feature in GitHub to get the [data file](https://github.com/Anth-us/semantic-text-classification/blob/main/data/labeled.csv) and then upload that as `labeled.csv` to the Colab environment.
2. Set a secret<Citation
  data={{
    type: "webpage",
    "container-title": "Medium",
    title: "How to use Secrets in Google Colab",
    author: "Parth Dasawant",
    URL: "https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75",
    issued: { 'date-parts': [[2023, 11, 2]] }
  }}
/> called `openai` and set the value to your OpenAI API token.  For calling the Ada 2 API.

You can also clone the [GitHub repository](https://github.com/Anth-us/semantic-text-classification) if you want to run it in Sagemaker or on your own machine or whatever.  It should run just about anywhere, with no special GPU power or anything.

You can also see an example of a text classifier built using function calling here in this [Colab notebook](https://colab.research.google.com/drive/1SVaxc3-d6n6JK2pcVlCRcuojNUhMd1cA?usp=sharing) or [on GitHub](https://github.com/Anth-us/LLM-classifier-example/blob/main/LLM_Classifier.ipynb).

## Evaluation metrics

We want to see how well the three different embeddings techniques work for our classifier.  We used these metrics:

**Accuracy**: This is the simplest measure, telling us the proportion of total predictions our model got right.<Citation
  data={{
    type: "webpage",
    "container-title": "Medium: Towards Data Science",
    title: "Metrics to Evaluate your Machine Learning Algorithm",
    author: "Aditya Mishra",
    URL: "https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234",
    issued: { 'date-parts': [[2018, 2, 24]] },
  }}
/> It's a quick way to gauge overall effectiveness.

**Precision and Recall**: These metrics offer a more nuanced view. Precision shows us how many of the sentences identified as questions about Spanish immigration law were actually so. Recall, on the other hand, tells us how many of the actual legal questions our model successfully identified.<Citation
  data={{
    type: "webpage",
    "container-title": "Iguazio",
    title: "What is Recall in Machine Learning?",
    URL: "https://www.iguazio.com/glossary/recall/",
    accessed: { 'date-parts': [[2018, 2, 24]] },
  }}
/>

**ROC-AUC Score**: This metric helps us understand the trade-offs between true positive rate and false positive rate.<Citation
  data={{
    type: "webpage",
    "container-title": "Medium: Towards Data Science",
    title: "How to Calculate & Use the AUC Score",
    author: "Nadim Kawwa",
    URL: "https://towardsdatascience.com/how-to-calculate-use-the-auc-score-1fc85c9a8430",
    issued: { 'date-parts': [[2020, 2, 9]] },
  }}
/> The AUC (Area Under the Curve) quantifies the model's ability to distinguish between classes - a higher AUC means better discrimination.

<BlogImage images={props.pageContext.frontmatter.images} name="ROC-curve.png" className="centered" alt="ROC-AUC curve analysis." />

**Confusion Matrices**: These are tables that lay out the successes and failures of our predictions in detail. They show four types of outcomes - true positives (correctly identified legal questions), false positives (non-legal questions wrongly identified as legal), true negatives (correctly identified non-legal questions), and false negatives (legal questions missed by the model).<Citation
  data={{
    type: "webpage",
    "container-title": "Medium: Towards Data Science",
    title: "Taking the Confusion Out of Confusion Matrices",
    author: "Allison Ragan",
    URL: "https://towardsdatascience.com/taking-the-confusion-out-of-confusion-matrices-c1ce054b3d3e",
    issued: { 'date-parts': [[2018, 10, 10]] },
  }}
/> This matrix helps us pinpoint areas where our model might be overconfident or too cautious, guiding us in fine-tuning its performance.

<BlogImage images={props.pageContext.frontmatter.images} name="confusion-BERT.png" className="centered" alt="Confusion matrix for BERT." />

## Maximize profit, not intelligence

You can do amazing things with OpenAI models but you don't always need them.
BERT is a widely-available, open algorithm that's so cheap and easy to run that you can compute 1,500 sentence embeddings from a free Google Colab notebook in a minute or two.  And it performs just as well as the OpenAI embeddings that you have to pay for.  Word2Vec embeddings, even cheaper to compute since the process is less computationally intense, also perform really well.  Not good enough for this example, but good enough for lots of things.

As we analyze these results, a broader business lesson emerges. In the AI arena, <mark>more powerful doesn't always mean more valuable.</mark> Just like our experiment, where BERT held its ground against mightier models, businesses need to gauge the cost-effectiveness of AI solutions.  With just a couple of iterations we turned a non-viable proof-of-concept into a viable business and then improved the profit by orders of magnitude.  <mark>The profit is the thing that matters</mark>, and that's what our iterations need to focus on.

## References

<CitationsList citationFormat="apa" />
